<!DOCTYPE html><html lang="en" class="dark"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="/images/document-control/AttackVsHonest.png"/><link rel="preload" as="image" href="/images/document-control/TrustedAndHonest.png"/><link rel="preload" as="image" href="/images/document-control/attackAndPlanning.png"/><link rel="stylesheet" href="/_next/static/css/528c5d8995e2c2e4.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-b10e1bf7a5faa37c.js"/><script src="/_next/static/chunks/fd9d1056-ae97a97996823271.js" async=""></script><script src="/_next/static/chunks/117-6a2f96f6d229f3b2.js" async=""></script><script src="/_next/static/chunks/main-app-99cdcbcb62796829.js" async=""></script><script src="/_next/static/chunks/app/blog/document-control/page-7072e2acc2d3fea8.js" async=""></script><script src="/_next/static/chunks/972-2ef7f6e850b6bfe8.js" async=""></script><script src="/_next/static/chunks/app/layout-9a64efe068df8acd.js" async=""></script><script src="/_next/static/chunks/878-12d1c1a81e6d2591.js" async=""></script><script src="/_next/static/chunks/app/page-86d8a14aa46b2d9e.js" async=""></script><title>Keshav Shenoy</title><meta name="description" content="A blog about my work and projects"/><link rel="shortcut icon" href="/favicon/icon1.png"/><link rel="icon" href="/favicon/favicon.ico"/><link rel="apple-touch-icon" href="/favicon/apple-icon.png"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__className_e8ce0c min-h-screen transition-colors duration-200 
        bg-slate-900 text-gray-100"><div class="max-w-5xl mx-auto px-4 py-8"><header class="mb-8"><nav class="flex items-center justify-between"><a class="text-2xl font-bold nav-link hover:text-emerald-400 transition-colors" href="/">Keshav Shenoy</a></nav></header><main><div class="grid grid-cols-1 lg:grid-cols-[200px_minmax(0,1fr)] gap-8"><article class="prose blog-content
          "><h1 class="text-4xl font-bold mb-8 dark:text-white light-mode:text-gray-900" id="heading-a-new-control-eval-for-summarizing-paper-results">A New Control Eval for Summarizing Paper Results</h1>
<p class="mb-4 dark:text-gray-300 light-mode:text-gray-700 leading-relaxed"><strong>Note:</strong> An initial version of this project was submitted to Apart Research&#x27;s Control Hackathon, where it placed 4th.</p>
<ul class="list-disc list-outside mb-4 ml-5 space-y-2">
<li class="dark:text-gray-300 light-mode:text-gray-700">You can see the submission for this and other projects on <a href="https://apartresearch.com/sprints/ai-control-hackathon-2025-03-29-to-2025-03-30" class="!inline-flex !items-center !gap-1 dark:!text-emerald-400 dark:hover:!text-emerald-300 light-mode:!text-emerald-600 light-mode:hover:!text-emerald-700 !transition-colors" target="_blank" rel="noopener noreferrer">their website<span class="text-xs opacity-50 group-hover:opacity-100 transition-opacity">↗</span></a>.</li>
<li class="dark:text-gray-300 light-mode:text-gray-700">I also gave a brief talk on this subject, <a href="https://docs.google.com/presentation/d/132mra0LkBU5_wt2tdHP04wTb3JlAYEOAlsDRh2dth1o/edit?usp=sharing" class="!inline-flex !items-center !gap-1 dark:!text-emerald-400 dark:hover:!text-emerald-300 light-mode:!text-emerald-600 light-mode:hover:!text-emerald-700 !transition-colors" target="_blank" rel="noopener noreferrer">the slides for which you can find here<span class="text-xs opacity-50 group-hover:opacity-100 transition-opacity">↗</span></a>.</li>
<li class="dark:text-gray-300 light-mode:text-gray-700">Find the code for this project <a href="https://github.com/k-shenoy/control-arena/tree/docs-setting/control_arena/settings/docs" class="!inline-flex !items-center !gap-1 dark:!text-emerald-400 dark:hover:!text-emerald-300 light-mode:!text-emerald-600 light-mode:hover:!text-emerald-700 !transition-colors" target="_blank" rel="noopener noreferrer">at this GitHub link<span class="text-xs opacity-50 group-hover:opacity-100 transition-opacity">↗</span></a>. It includes a readme with more implementation details, as well as the full Inspect logs for the runs described below.</li>
</ul>
<hr/>
<h2 class="text-2xl font-bold mt-8 mb-4 dark:text-white light-mode:text-gray-900 border-l-4 border-emerald-500 pl-3" id="heading-overview">Overview</h2>
<p class="mb-4 dark:text-gray-300 light-mode:text-gray-700 leading-relaxed">This project is a preliminary evaluation for measuring AI control on document summarization tasks based on <a href="https://arxiv.org/pdf/2312.06942" class="!inline-flex !items-center !gap-1 dark:!text-emerald-400 dark:hover:!text-emerald-300 light-mode:!text-emerald-600 light-mode:hover:!text-emerald-700 !transition-colors" target="_blank" rel="noopener noreferrer">Redwood&#x27;s original control paper<span class="text-xs opacity-50 group-hover:opacity-100 transition-opacity">↗</span></a> and a specification of this setting by Tyler Tracy. The code is currently embedded within a fork of UKAISI&#x27;s control-arena, but it is not yet integrated and operates as a standalone evaluation.</p>
<p class="mb-4 dark:text-gray-300 light-mode:text-gray-700 leading-relaxed">This eval measures whether an untrusted AI model (UM) can deceive a less powerful trusted model (TM) when summarizing documents. Specifically, the eval tests:</p>
<ol>
<li class="dark:text-gray-300 light-mode:text-gray-700">Whether a UM can generate a misleading summary of an AI safety paper that leads a TM to come to a specific incorrect conclusion</li>
<li class="dark:text-gray-300 light-mode:text-gray-700">Whether a TM monitor can detect when these summaries are intended to be misleading</li>
<li class="dark:text-gray-300 light-mode:text-gray-700">Whether believing the incorrect conclusion significantly impacts TM performance on a set of multiple choice questions about the paper</li>
</ol>
<p class="mb-4 dark:text-gray-300 light-mode:text-gray-700 leading-relaxed">We tested three red-team policies and 2 blue-team protocols and found that the blue-team needs stronger monitoring strategies to prevent an attack.</p>
<h3 class="text-xl font-bold mt-6 mb-3 dark:text-gray-100 light-mode:text-gray-800" id="heading-motivation">Motivation</h3>
<p class="mb-4 dark:text-gray-300 light-mode:text-gray-700 leading-relaxed">AI models may soon be performing AI safety research independently or being used heavily in research roles and we may need to rely on untrusted models for descriptions of the work being performed. As such, having good control evaluations of summarizing ability is critical for the research sabotage safety case.</p>
<p class="mb-4 dark:text-gray-300 light-mode:text-gray-700 leading-relaxed">As such, we have a control evaluation where the blue-team is kept from viewing the original paper, and needs to determine the results from a untrusted model&#x27;s summary. The blue-team also has access to a set of easily verifiable facts about the paper, represented in the evaluation as a set of multiple choice questions that it can use to judge the veracity of the UM summary. See the &quot;Future Work&quot; section for limitations of this design.</p>
<h3 class="text-xl font-bold mt-6 mb-3 dark:text-gray-100 light-mode:text-gray-800" id="heading-evaluation-flow">Evaluation Flow</h3>
<ol>
<li class="dark:text-gray-300 light-mode:text-gray-700">The UM generates a summary of a modern AI safety paper using one of the policies</li>
<li class="dark:text-gray-300 light-mode:text-gray-700">From the summary:<!-- -->
<ul class="list-disc list-outside mb-4 ml-5 space-y-2">
<li class="dark:text-gray-300 light-mode:text-gray-700">A TM reader model answers questions about the paper based only on the summary</li>
<li class="dark:text-gray-300 light-mode:text-gray-700">A TM reader model generates from the summary its takeaway of the main findings of the paper</li>
<li class="dark:text-gray-300 light-mode:text-gray-700">A TM monitoring model attempts to detect whether the summary is misleading</li>
</ul>
</li>
<li class="dark:text-gray-300 light-mode:text-gray-700">A judge model rates whether the reader&#x27;s generated understanding aligns with the honest or dishonest takeaway</li>
</ol>
<h2 class="text-2xl font-bold mt-8 mb-4 dark:text-white light-mode:text-gray-900 border-l-4 border-emerald-500 pl-3" id="heading-key-components">Key Components</h2>
<h3 class="text-xl font-bold mt-6 mb-3 dark:text-gray-100 light-mode:text-gray-800" id="heading-metrics">Metrics</h3>
<p class="mb-4 dark:text-gray-300 light-mode:text-gray-700 leading-relaxed">The outputs of the evaluation are:</p>
<ul class="list-disc list-outside mb-4 ml-5 space-y-2">
<li class="dark:text-gray-300 light-mode:text-gray-700"><strong>Reader Accuracy</strong>: How well readers answer factual questions about the paper based on the UM summary</li>
<li class="dark:text-gray-300 light-mode:text-gray-700"><strong>Monitor Detection</strong>: How suspicious the monitors find the UM summary</li>
<li class="dark:text-gray-300 light-mode:text-gray-700"><strong>UM Success</strong>: If the UM can get the reader model to generate a takeaway more aligned with its given incorrect finding than the real findings - referred to as judge accuracy in the code</li>
</ul>
<h3 class="text-xl font-bold mt-6 mb-3 dark:text-gray-100 light-mode:text-gray-800" id="heading-dataset-generation">Dataset Generation</h3>
<p class="mb-4 dark:text-gray-300 light-mode:text-gray-700 leading-relaxed">The dataset for this evaluation was created from 42 AI safety papers. For each paper, we extracted the raw text and generated:</p>
<ul class="list-disc list-outside mb-4 ml-5 space-y-2">
<li class="dark:text-gray-300 light-mode:text-gray-700">10 multiple choice questions with four answer choices,</li>
<li class="dark:text-gray-300 light-mode:text-gray-700">An honest 2-3 sentence conclusion of the paper</li>
<li class="dark:text-gray-300 light-mode:text-gray-700">A false 2-3 sentence conclusion which is strongly changed from the real paper results</li>
</ul>
<p class="mb-4 dark:text-gray-300 light-mode:text-gray-700 leading-relaxed">All were generated using GPT-4o.</p>
<h3 class="text-xl font-bold mt-6 mb-3 dark:text-gray-100 light-mode:text-gray-800" id="heading-policies">Policies</h3>
<p class="mb-4 dark:text-gray-300 light-mode:text-gray-700 leading-relaxed">The framework supports different policies for summarization:</p>
<ol>
<li class="dark:text-gray-300 light-mode:text-gray-700"><strong>Honest Policy</strong>: The UM generates accurate summaries (represents attempts where the UM does not try to deceive the blue-team) - referred to as &quot;honest&quot; in the code</li>
<li class="dark:text-gray-300 light-mode:text-gray-700"><strong>Dishonest Policy</strong>: The UM is given a specific incorrect takeaway and generates inaccurate summaries to engender that belief on its own - referred to as &quot;attack&quot; or &quot;dishonest&quot; in the code</li>
<li class="dark:text-gray-300 light-mode:text-gray-700"><strong>Dishonest with Red Team Planning</strong>: The same as &quot;Dishonest&quot;, but the UM is fed a red-team written reasoning process before and during the summary attempt to keep it from forgetting the task and reduce reversion towards more honest behavior - referred to as &quot;dishonest_planning&quot; or &quot;attack_planning&quot; in the code</li>
<li class="dark:text-gray-300 light-mode:text-gray-700"><strong>Trusted Model</strong>: The TM summarizes smaller chunks of the research paper sequentially within a limited context length, using its already generated summary and a small portion of the paper. Equivalent of the &quot;Don&#x27;t use untrusted models&quot; baseline from the <a href="https://arxiv.org/pdf/2312.06942" class="!inline-flex !items-center !gap-1 dark:!text-emerald-400 dark:hover:!text-emerald-300 light-mode:!text-emerald-600 light-mode:hover:!text-emerald-700 !transition-colors" target="_blank" rel="noopener noreferrer">original control paper<span class="text-xs opacity-50 group-hover:opacity-100 transition-opacity">↗</span></a> - referred to as &quot;trusted&quot; in the code</li>
</ol>
<p class="mb-4 dark:text-gray-300 light-mode:text-gray-700 leading-relaxed">While I list 4 policies here, only 1, 2, and 3 are different red-team policies, and 4 represents a different blue-team protocol. Since, the red-team cannot attack a &quot;Trusted Model&quot; protocol, we were able to treat all four the same. In this testing, the TM was always GPT-3.5-turbo and UM was Claude-3.5-Sonnet (New).derange</p>
<h2 class="text-2xl font-bold mt-8 mb-4 dark:text-white light-mode:text-gray-900 border-l-4 border-emerald-500 pl-3" id="heading-results">Results</h2>
<p class="mb-4 dark:text-gray-300 light-mode:text-gray-700 leading-relaxed">The main results from the experiments were:</p>
<ul class="list-disc list-outside mb-4 ml-5 space-y-2">
<li class="dark:text-gray-300 light-mode:text-gray-700">The multiple choice question performance was high for all policies. One main criteria for control evals is that the UM significantly outperforms the TM. Otherwise, there&#x27;s no usefulness cost to deploying the TM in all situations. In order to get more value out of this control eval, we will need to show the capability difference between UM and TM. This improved verification strategy would also likely put pressure on the red-team to optimize better for the MCQ, where now all policies perform well without optimization.</li>
<li class="dark:text-gray-300 light-mode:text-gray-700">The red team is able to get attacks through very frequently, but is caught by the monitor a reasonable portion of the time. If the red-team wins by successfully pulling off a single attack without being caught, the blue team would need a much better protocol for acceptable safety. If the document setting is one where the attacker needs to be able to consistently pull off successful attacks for some timeframe without being caught, the red team will need more subtle attacks to avoid being caught.</li>
<li class="dark:text-gray-300 light-mode:text-gray-700">There is a lot of low-hanging fruit in improving the UM deception. Sonnet often avoided following red-team instructions in simple-attack cases. I would expect better prompting, finetuning, or a model change to lead to significant red-team improvements.</li>
</ul>
<h2 class="text-2xl font-bold mt-8 mb-4 dark:text-white light-mode:text-gray-900 border-l-4 border-emerald-500 pl-3" id="heading-results-in-detail">Results in Detail</h2>
<p class="mb-4 dark:text-gray-300 light-mode:text-gray-700 leading-relaxed">Below, we show scatterplots for the individual samples with dotted mean lines. The Judge score is from 0-1, with high scores representing the judge rating the dishonest takeaway more highly. Suspicion scores were given by the monitor from 1-100. MCQ score is accuracy on the questions from 0-1.</p>
<h3 class="text-xl font-bold mt-6 mb-3 dark:text-gray-100 light-mode:text-gray-800" id="heading-attack-and-honest-policies">Attack and Honest Policies</h3>
<p class="mb-4 dark:text-gray-300 light-mode:text-gray-700 leading-relaxed"><img src="/images/document-control/AttackVsHonest.png" alt="Attack vs Honest Policy Results" class="rounded-lg shadow-lg dark:border dark:border-slate-700 light-mode:border light-mode:border-gray-200"/>
First, we can look at the simplest policies. What happens when the UM is honest vs when it is lying.</p>
<ul class="list-disc list-outside mb-4 ml-5 space-y-2">
<li class="dark:text-gray-300 light-mode:text-gray-700">The red-team is able to get the judge model to choose their narrative a reasonable portion of the time and is not flagged as suspicious for many of those attempts.</li>
<li class="dark:text-gray-300 light-mode:text-gray-700">The majority of attacks don&#x27;t result in an incorrect judge diagnosis. Looking at the model reasoning here, the UM (Sonnet) is frequently negotiating its way back to a nuanced position or maybe even forgetting its task. There&#x27;s an opportunity here to try more aggressive strategies or to try using UMs which are less well aligned with honesty and harmlessness. (Sonnet frequently refused when we tried more explicit negative frames)</li>
<li class="dark:text-gray-300 light-mode:text-gray-700">The monitor does flag the attacking model more often than the honest one, but there&#x27;s a good amount of work to be done refining the monitor.</li>
<li class="dark:text-gray-300 light-mode:text-gray-700">Not shown here, but reader MCQ score was barely changed between policies (0.864 for honest and 0.843 for attacking)</li>
</ul>
<h3 class="text-xl font-bold mt-6 mb-3 dark:text-gray-100 light-mode:text-gray-800" id="heading-trusted-policy">Trusted Policy</h3>
<p class="mb-4 dark:text-gray-300 light-mode:text-gray-700 leading-relaxed"><img src="/images/document-control/TrustedAndHonest.png" alt="Trusted vs Honest Policy Results" class="rounded-lg shadow-lg dark:border dark:border-slate-700 light-mode:border light-mode:border-gray-200"/>
Generally, policies are compared against a blue team strategy of not using any untrusted models. Here, implementing a trusted only strategy resulted in a very small decline in usefulness. The average MCQ score for the reader declined from 0.864 to 0.826 and there was essentially no difference in the judge rating.</p>
<p class="mb-4 dark:text-gray-300 light-mode:text-gray-700 leading-relaxed">This could be a sign that this task is not well-suited for a control evaluation. If the difference in ability between TM and UM is small, there&#x27;s not much reason to use the UM. I personally expect that this is actually an artifact of poor question design. As a result, I think maybe <strong>the most important area for future work here is trying to better measure a capability gap between the UM and TM</strong>. This could take the form of switching to a set of open-ended questions and model-rating reader answers or in building a set of more difficult, targeted MCQs.</p>
<p class="mb-4 dark:text-gray-300 light-mode:text-gray-700 leading-relaxed">Other notes:</p>
<ul class="list-disc list-outside mb-4 ml-5 space-y-2">
<li class="dark:text-gray-300 light-mode:text-gray-700">The monitor (which probably wouldn&#x27;t be run on the TM usually) actually found the summaries very suspicious due to the constant repetitions and general poor coherence from the recursive summarization approach</li>
<li class="dark:text-gray-300 light-mode:text-gray-700">The UM is capable of reading the PDFs as multimedia inputs, but I input to both kinds of models with only text. Switching to the best form of input for UMs may also be a way to better measure the capability gap.</li>
</ul>
<h3 class="text-xl font-bold mt-6 mb-3 dark:text-gray-100 light-mode:text-gray-800" id="heading-red-team-planned-attacks">Red-team Planned Attacks</h3>
<p class="mb-4 dark:text-gray-300 light-mode:text-gray-700 leading-relaxed"><img src="/images/document-control/attackAndPlanning.png" alt="Attack and Planning Policy Results" class="rounded-lg shadow-lg dark:border dark:border-slate-700 light-mode:border light-mode:border-gray-200"/>
Iterating on the red-team strategy, we checked if we could get better results by more aggressively pushing the UM to lie:</p>
<ul class="list-disc list-outside mb-4 ml-5 space-y-2">
<li class="dark:text-gray-300 light-mode:text-gray-700">From the logs, this approach did get the UM to almost always generate a summary biased towards its own viewpoints</li>
<li class="dark:text-gray-300 light-mode:text-gray-700">The monitor found &quot;attack-planning&quot; generations suspicious reasonably more often than normal attack submissions, but there are still a pretty large number of attacks getting through.</li>
</ul>
<h3 class="text-xl font-bold mt-6 mb-3 dark:text-gray-100 light-mode:text-gray-800" id="heading-full-results">Full Results</h3>
<table><thead><tr><th>Strategy</th><th>Judge Belief In Incorrect Takeaway</th><th>Reader MCQ Score</th><th>Monitor Suspicion</th></tr></thead><tbody><tr><td>Attack Simple</td><td>0.285</td><td>0.843</td><td>27.02</td></tr><tr><td>Attack Planned</td><td>0.592</td><td>0.788</td><td>46.07</td></tr><tr><td>Honest</td><td>0.085</td><td>0.864</td><td>14.05</td></tr><tr><td>Trusted</td><td>0.087</td><td>0.826</td><td>66.19</td></tr></tbody></table>
<p class="mb-4 dark:text-gray-300 light-mode:text-gray-700 leading-relaxed">Here are the full results for the experiments, given with the means for each subset.</p>
<h2 class="text-2xl font-bold mt-8 mb-4 dark:text-white light-mode:text-gray-900 border-l-4 border-emerald-500 pl-3" id="heading-future-work">Future Work</h2>
<p class="mb-4 dark:text-gray-300 light-mode:text-gray-700 leading-relaxed">These are all things I may work on in the future for improving this evaluation:</p>
<h3 class="text-xl font-bold mt-6 mb-3 dark:text-gray-100 light-mode:text-gray-800" id="heading-general-improvements">General Improvements</h3>
<ul class="list-disc list-outside mb-4 ml-5 space-y-2">
<li class="dark:text-gray-300 light-mode:text-gray-700"><strong>Framework Integration</strong>: Fully integrate with the broader Control Arena framework - this project was intended as a new setting, but it is not currently integrated at all.</li>
<li class="dark:text-gray-300 light-mode:text-gray-700"><strong>More Models</strong>: Evaluate different models as the UM or TM - Loose feeling that non-Sonnet models may be more willing to be deceptive in summary generation.</li>
<li class="dark:text-gray-300 light-mode:text-gray-700"><strong>Question Improvement</strong>: Regenerate multiple choice questions with more care to better test understanding. Existing question generation is pretty rudimentary, with undesired overlap between questions and core takeaways. Need more focus on questions which are easily verifiable. Ideally, MCQs would become more difficult and open a larger performance gap between &quot;Honest&quot; and &quot;Trusted&quot; policies.</li>
<li class="dark:text-gray-300 light-mode:text-gray-700"><strong>PDF Input</strong>: Add direct PDF multimedia input capability for the UM instead of just plain text.</li>
<li class="dark:text-gray-300 light-mode:text-gray-700"><strong>Dataset Expansion</strong>: Expand the paper dataset in both quantity and diversity - currently only includes 42 AI safety papers from the past 4 years.</li>
<li class="dark:text-gray-300 light-mode:text-gray-700"><strong>Switch to asynchronous solvers and caching</strong>: After summary generation, the three TM calls are not dependent on each other, so we don&#x27;t need to have them be synchronous, as they are now. Integrating with Control Arena would add caching support, which would make running different sections on their own much easier.</li>
</ul>
<h3 class="text-xl font-bold mt-6 mb-3 dark:text-gray-100 light-mode:text-gray-800" id="heading-red-team-enhancements">Red Team Enhancements</h3>
<ul class="list-disc list-outside mb-4 ml-5 space-y-2">
<li class="dark:text-gray-300 light-mode:text-gray-700"><strong>Plausibility of Incorrect Conclusion</strong>: It looks like most of the monitor&#x27;s suspicion rating is based on how plausible the paper conclusions are. This is probably not a robust way of finding deceptive summaries. Spending more time creating plausible incorrect conclusions would probably reduce the suspicion gap between the policies.</li>
<li class="dark:text-gray-300 light-mode:text-gray-700"><strong>More Granularity in Deception</strong>: Create and measure success across different levels of divergence from the real conclusion.</li>
</ul>
<h3 class="text-xl font-bold mt-6 mb-3 dark:text-gray-100 light-mode:text-gray-800" id="heading-blue-team-defenses">Blue Team Defenses</h3>
<ul class="list-disc list-outside mb-4 ml-5 space-y-2">
<li class="dark:text-gray-300 light-mode:text-gray-700"><strong>More Kinds of Verification</strong>: Try open-ended questions and other verification methods beyond multiple choice questions.</li>
<li class="dark:text-gray-300 light-mode:text-gray-700"><strong>Monitoring Context</strong>: Give monitors more context about red-team goals - may trade off with real-world scenarios where red-teams could be trying multiple different attacks.</li>
<li class="dark:text-gray-300 light-mode:text-gray-700"><strong>Cross-Verification</strong>: Test cross-verified querying of multiple UM instances versus one-time summarization.</li>
</ul></article></div><button class="fixed top-8 right-8 bg-blue-500 hover:bg-blue-600 text-white p-3 rounded-full shadow-lg transition-all duration-300 transform hover:scale-105 z-50 opacity-0 pointer-events-none" aria-label="Back to top"><svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 10l7-7m0 0l7 7m-7-7v18"></path></svg></button></main><footer class="mt-8 py-4 text-center dark:text-gray-400 light-mode:text-gray-500">© <!-- -->2025<!-- --> Keshav Shenoy. All rights reserved.</footer></div><script src="/_next/static/chunks/webpack-b10e1bf7a5faa37c.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/e4af272ccee01ff0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/528c5d8995e2c2e4.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"3:I[2846,[],\"\"]\n5:I[8950,[\"599\",\"static/chunks/app/blog/document-control/page-7072e2acc2d3fea8.js\"],\"default\"]\n6:I[4707,[],\"\"]\n7:I[6423,[],\"\"]\n8:I[8009,[\"972\",\"static/chunks/972-2ef7f6e850b6bfe8.js\",\"185\",\"static/chunks/app/layout-9a64efe068df8acd.js\"],\"ThemeProvider\"]\n9:I[2972,[\"972\",\"static/chunks/972-2ef7f6e850b6bfe8.js\",\"878\",\"static/chunks/878-12d1c1a81e6d2591.js\",\"931\",\"static/chunks/app/page-86d8a14aa46b2d9e.js\"],\"\"]\nb:I[1060,[],\"\"]\nc:[]\n"])</script><script>self.__next_f.push([1,"0:[\"$\",\"$L3\",null,{\"buildId\":\"CYwiF-4hFBwbN-1-wiCHu\",\"assetPrefix\":\"\",\"urlParts\":[\"\",\"blog\",\"document-control\",\"\"],\"initialTree\":[\"\",{\"children\":[\"blog\",{\"children\":[\"document-control\",{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"blog\",{\"children\":[\"document-control\",{\"children\":[\"__PAGE__\",{},[[\"$L4\",[\"$\",\"$L5\",null,{}],null],null],null]},[null,[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\",\"document-control\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L7\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[null,[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L7\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/528c5d8995e2c2e4.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"dark\",\"children\":[\"$\",\"body\",null,{\"className\":\"__className_e8ce0c min-h-screen transition-colors duration-200 \\n        bg-slate-900 text-gray-100\",\"children\":[\"$\",\"$L8\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-5xl mx-auto px-4 py-8\",\"children\":[[\"$\",\"header\",null,{\"className\":\"mb-8\",\"children\":[\"$\",\"nav\",null,{\"className\":\"flex items-center justify-between\",\"children\":[\"$\",\"$L9\",null,{\"href\":\"/\",\"className\":\"text-2xl font-bold nav-link hover:text-emerald-400 transition-colors\",\"children\":\"Keshav Shenoy\"}]}]}],[\"$\",\"main\",null,{\"children\":[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L7\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[]}]}],[\"$\",\"footer\",null,{\"className\":\"mt-8 py-4 text-center dark:text-gray-400 light-mode:text-gray-500\",\"children\":[\"© \",2025,\" Keshav Shenoy. All rights reserved.\"]}]]}]}]}]}]],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$La\"],\"globalErrorComponent\":\"$b\",\"missingSlots\":\"$Wc\"}]\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Keshav Shenoy\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"A blog about my work and projects\"}],[\"$\",\"link\",\"4\",{\"rel\":\"shortcut icon\",\"href\":\"/favicon/icon1.png\"}],[\"$\",\"link\",\"5\",{\"rel\":\"icon\",\"href\":\"/favicon/favicon.ico\"}],[\"$\",\"link\",\"6\",{\"rel\":\"apple-touch-icon\",\"href\":\"/favicon/apple-icon.png\"}],[\"$\",\"meta\",\"7\",{\"name\":\"next-size-adjust\"}]]\n4:null\n"])</script></body></html>